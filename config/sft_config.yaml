# 预训练配置文件

# Tokenizer路径
tokenizer_path: "tokenizer/minimind"
dtype: float16

# 模型配置
model:
  vocab_size: 6400
  n_layers: 12
  d_model: 768
  q_head: 16
  kv_head: 4
  d_ff: 3072
  dropout_p: 0.1
  max_seq_length: 512
  rope_theta: 1000000.0

# 数据配置
data:
  train_data_path: ["data/sft_512.jsonl"]
  max_length: 512  # 改为512，与模型一致
  num_workers: 4              # 增加到CPU核心数的1/2到2/3
  prefetch_factor: 2           # 增加预取buffer，从2改到4
  persistent_workers: true     # 添加这行：保持worker进程不重启
  pin_memory: true       # 启用pin_memory加速数据传输
  drop_last: true        # 丢弃最后不完整的batch，保证各GPU负载均衡

# 训练配置
trainer:
  num_epochs: 1
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_steps: 500               # 增加warmup步数，更平滑
  max_steps: -1                    # -1表示使用num_epochs

  batch_size: 128
  gradient_accumulation_steps: 2

  # 日志和保存
  eval_steps: 1000                  # 每多少步评估一次
  save_steps: 10000                 # 每多少步保存一次
  logging_steps: 1                  # 每多少步记录日志
  
  # 其他配置
  gradient_checkpointing: false     # 显存充足，关闭checkpointing以换取更快速度
  max_grad_norm: 1.0                # 梯度裁剪
  seed: 42
  enable_timing: true               # 启用性能计时（记录到WandB）


  
# 输出配置
output:
  output_dir: "output/sft"
#  save_total_limit: 5
  resume_from_checkpoint: "output/pretrain/epoch-4.pth"  # 从预训练模型开始微调
  continue_train: false  # false 表示只加载模型权重，从 epoch 0 开始训练
  
# WandB配置
wandb:
  enabled: true
  project: "zx-llm-sft"
  run_name: "sft"
  entity: null                     # WandB用户名或团队名

